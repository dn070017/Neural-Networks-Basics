{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "num_epochs  = 10\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.random.normal((num_samples, 2), mean=0, stddev=1)\n",
    "y = tf.reshape(3.145 * x[:, 0] +  6.323 * x[:, 1], (-1, 1))\n",
    "\n",
    "square_loss = lambda: tf.reduce_mean(tf.pow(y - tf.matmul(x, w), 2))\n",
    "evaluate = lambda i: print(''.join((\n",
    "    f'epoch:{i+1:>3}{\"\":>4}weight 1: {w.numpy()[0, 0]:>7.3f}',\n",
    "    f'{\"\":>4}weight 2: {w.numpy()[1, 0]:>7.3f}{\"\":>4}loss: {square_loss().numpy():>7.3f}'\n",
    ")))\n",
    "\n",
    "def train(num_epochs, optimizer, loss, weight):\n",
    "    evaluate(-1)\n",
    "    for i in range(0, num_epochs):\n",
    "        optimizer.minimize(loss, var_list=[w])\n",
    "        evaluate(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Gradient Descent (AdaGrad)\n",
    "* Equation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{(t+1)}&=\\mathbf{w}^{(t)}-\\frac{\\eta}{\\sqrt{\\sum\\limits_{i=1}^t\\mathbf{g}^{(t)T}\\mathbf{g}^{(t)}}+\\epsilon} \\mathbf{g}^{(t)}\\\\\n",
    "\\mathbf{g}^{(t)}&=âˆ‡_\\mathbf{w}L(\\mathbf{w}^{(t)})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "* Parameters\n",
    "    * $\\epsilon$ is a constant that make sure the denominator is not zero (default in Tensorflow/Keras $1e-7$).\n",
    "    * $\\eta$ is the learning rate (default in Tensorflow/Keras $0.001$)\n",
    "\n",
    "* Properties\n",
    "    * The magnitude of the update is smaller in later iterations.\n",
    "    * Converge even with large learning rate.\n",
    "    * The update become very inefficient in later iterations.\n",
    "    * Vunerable to local minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0    weight 1:   0.000    weight 2:   0.000    loss:  37.729\n",
      "epoch:  1    weight 1:   0.999    weight 2:   0.999    loss:  23.809\n",
      "epoch:  2    weight 1:   1.574    weight 2:   1.639    loss:  17.052\n",
      "epoch:  3    weight 1:   1.973    weight 2:   2.126    loss:  12.899\n",
      "epoch:  4    weight 1:   2.267    weight 2:   2.523    loss:  10.088\n",
      "epoch:  5    weight 1:   2.492    weight 2:   2.860    loss:   8.075\n",
      "epoch:  6    weight 1:   2.666    weight 2:   3.152    loss:   6.577\n",
      "epoch:  7    weight 1:   2.801    weight 2:   3.410    loss:   5.430\n",
      "epoch:  8    weight 1:   2.907    weight 2:   3.639    loss:   4.530\n",
      "epoch:  9    weight 1:   2.989    weight 2:   3.844    loss:   3.812\n",
      "epoch: 10    weight 1:   3.053    weight 2:   4.030    loss:   3.229\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "opt = optimizers.Adagrad(learning_rate=eta)\n",
    "train(num_epochs, opt, square_loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "* Equation\n",
    "$$\n",
    "\\begin{align}\\mathbf{w}^{(t+1)}&=\\mathbf{w}^{(t)}-\\frac{\\eta}{\\sqrt{G^{(t)}} + \\epsilon}\\mathbf{g}^{(t)}\\\\\n",
    "\\mathbf{g}^{(t)}&=\\nabla_\\mathbf{w}L(\\mathbf{w}^{(t)})\\\\\n",
    "G^{(t)}&=\\rho G^{(t-1)}+(1-\\rho)\\mathbf{g}^{(t)T}\\mathbf{g}^{(t)}\n",
    "\\end{align}\n",
    "$$\n",
    "* Parameters\n",
    "    * $\\epsilon$ is a constant that make sure the denominator is not zero (default in Tensorflow/Keras $1e-7$).\n",
    "    * $\\rho$ is the parameter that control how much the model should consider the computed gradient (sum of magnitude) before. (default in Tensorflow/Keras $0.9$)\n",
    "    * $\\eta$ is the learning rate (default in Tensorflow/Keras $0.001$)\n",
    "\n",
    "* Properties\n",
    "    * Converge even with large learning rate.\n",
    "    * Vunerable to local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0    weight 1:   0.000    weight 2:   0.000    loss:  37.729\n",
      "epoch:  1    weight 1:   3.162    weight 2:   3.162    loss:   6.049\n",
      "epoch:  2    weight 1:   3.368    weight 2:   4.563    loss:   1.867\n",
      "epoch:  3    weight 1:   3.282    weight 2:   5.347    loss:   0.576\n",
      "epoch:  4    weight 1:   3.218    weight 2:   5.800    loss:   0.165\n",
      "epoch:  5    weight 1:   3.182    weight 2:   6.055    loss:   0.043\n",
      "epoch:  6    weight 1:   3.163    weight 2:   6.193    loss:   0.010\n",
      "epoch:  7    weight 1:   3.153    weight 2:   6.263    loss:   0.002\n",
      "epoch:  8    weight 1:   3.149    weight 2:   6.297    loss:   0.000\n",
      "epoch:  9    weight 1:   3.146    weight 2:   6.313    loss:   0.000\n",
      "epoch: 10    weight 1:   3.146    weight 2:   6.319    loss:   0.000\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "opt = optimizers.RMSprop(learning_rate=eta)\n",
    "train(num_epochs, opt, square_loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momemtum\n",
    "* Equation\n",
    "$$\n",
    "\\begin{align}\\mathbf{w}^{(t+1)}&=\\mathbf{w}^{(t)}-\\mathbf{v}^{(t)}\\\\\n",
    "\\mathbf{v}^{(t)}&=\\gamma\\mathbf{v}^{(t-1)}+\\eta\\nabla_\\mathbf{w}L(\\mathbf{w}^{(t)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Parameters\n",
    "    * $\\gamma$ is the momemteum parameter that control how much the model should consider the computed gradient before.\n",
    "\n",
    "* Properties\n",
    "    * Implemented in `SGD` and `RMSProp` in Tensorflow and Keras.\n",
    "    * Can generalize well to most of the problems.\n",
    "    * Consider both magnitude and direction of previous gradient when update the parameters.\n",
    "    * Might not converge with large learning rate.\n",
    "    * Less vunerable to local minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0    weight 1:   0.000    weight 2:   0.000    loss:  37.729\n",
      "epoch:  1    weight 1:   0.757    weight 2:   0.817    loss:  26.427\n",
      "epoch:  2    weight 1:   1.419    weight 2:   1.604    loss:  17.908\n",
      "epoch:  3    weight 1:   1.920    weight 2:   2.283    loss:  12.251\n",
      "epoch:  4    weight 1:   2.290    weight 2:   2.860    loss:   8.505\n",
      "epoch:  5    weight 1:   2.561    weight 2:   3.351    loss:   5.987\n",
      "epoch:  6    weight 1:   2.757    weight 2:   3.770    loss:   4.266\n",
      "epoch:  7    weight 1:   2.898    weight 2:   4.128    loss:   3.072\n",
      "epoch:  8    weight 1:   2.999    weight 2:   4.434    loss:   2.230\n",
      "epoch:  9    weight 1:   3.069    weight 2:   4.696    loss:   1.631\n",
      "epoch: 10    weight 1:   3.118    weight 2:   4.921    loss:   1.199\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "opt = optimizers.SGD(learning_rate=eta, momentum=0.1)\n",
    "train(num_epochs, opt, square_loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerate Gradient (NAG)\n",
    "* Equation\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{(t+1)}&=\\mathbf{v}^{(t+1)}-\\eta\\nabla_\\mathbf{v}L(\\mathbf{v}^{(t)})\\\\\n",
    "\\mathbf{v}^{(t+1)}&=\\mathbf{w}^{(t)}+\\gamma(\\mathbf{w}^{(t)}-\\mathbf{w}^{(t-1)})\n",
    "\\end{align}\n",
    "* Parameters\n",
    "    * $\\gamma$ is the momemteum parameter that control how much the model should consider the computed gradient before.\n",
    "\n",
    "* Properties\n",
    "    * Implemented in `SGD` in Tensorflow and Keras.\n",
    "    * Can generalize well to most of the problems.\n",
    "    * Consider both magnitude and direction of previous gradient when update the parameters.\n",
    "    * Gradients in earlier iterations become less informative, and therefore usually reach convergence faster than momemtum.\n",
    "    * Might not converge with large learning rate.\n",
    "    * Less vunerable to local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0    weight 1:   0.000    weight 2:   0.000    loss:  37.729\n",
      "epoch:  1    weight 1:   0.833    weight 2:   0.899    loss:  25.418\n",
      "epoch:  2    weight 1:   1.466    weight 2:   1.671    loss:  17.308\n",
      "epoch:  3    weight 1:   1.939    weight 2:   2.329    loss:  11.958\n",
      "epoch:  4    weight 1:   2.291    weight 2:   2.889    loss:   8.377\n",
      "epoch:  5    weight 1:   2.551    weight 2:   3.368    loss:   5.943\n",
      "epoch:  6    weight 1:   2.742    weight 2:   3.777    loss:   4.262\n",
      "epoch:  7    weight 1:   2.881    weight 2:   4.128    loss:   3.086\n",
      "epoch:  8    weight 1:   2.981    weight 2:   4.429    loss:   2.252\n",
      "epoch:  9    weight 1:   3.053    weight 2:   4.688    loss:   1.654\n",
      "epoch: 10    weight 1:   3.104    weight 2:   4.910    loss:   1.221\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "opt = optimizers.SGD(learning_rate=eta, momentum=0.1, nesterov=True)\n",
    "train(num_epochs, opt, square_loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "* Equation\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{(t+1)}&=\\mathbf{w}^{(t)}-\\frac{\\eta}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon}\\mathbf{\\hat{m}}^{(t)}\\\\\n",
    "\\mathbf{g}^{(t)}&=\\nabla_\\mathbf{w}L(\\mathbf{w}^{(t)})\\\\\n",
    "G^{(t)}&=\\mathbf{g}^{(t)T}\\mathbf{g}^{(t)}\\\\\n",
    "\\mathbf{m}^{(t)}&=\\beta_1\\mathbf{m}^{(t-1)}+(1-\\beta_1)\\mathbf{g}^{(t)}\\\\\n",
    "v^{(t)}&=\\beta_2v^{(t-1)}+(1-\\beta_2)G^{(t)}\\\\\n",
    "\\mathbf{\\hat{m}}^{(t)}&=\\frac{\\mathbf{m^{(t)}}}{1-\\beta_1^t}\\\\\n",
    "v^{(t)}&=\\frac{v^{(t)}}{1-\\beta_2^t}\\\\\n",
    "\\end{align}\n",
    "* Parameters\n",
    "    * $\\epsilon$ is a constant that make sure the denominator is not zero. (default in Tensorflow/Keras $1e-7$).\n",
    "    * $\\beta_1$ is the parameter that control how much the model should consider the computed gradient (consider both direction and magnitude) before. (default in Tensorflow/Keras $0.9$)\n",
    "    * $\\beta_2$ is the parameter that control how much the model should consider the computed gradient (sum of magnitude) before. (default in Tensorflow/Keras $0.999$)\n",
    "    * $\\eta$ is the learning rate (default in Tensorflow/Keras $0.001$)\n",
    "* Properties\n",
    "    * Consider both magnitude and direction of previous gradient when update the parameters.\n",
    "    * Usually converge slower than momentum and NAG.\n",
    "    * Converge even with large learning rate.\n",
    "    * Less vunerable to local minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0    weight 1:   0.000    weight 2:   0.000    loss:  37.729\n",
      "epoch:  1    weight 1:   1.000    weight 2:   1.000    loss:  23.800\n",
      "epoch:  2    weight 1:   1.976    weight 2:   1.991    loss:  13.610\n",
      "epoch:  3    weight 1:   2.896    weight 2:   2.964    loss:   7.039\n",
      "epoch:  4    weight 1:   3.707    weight 2:   3.907    loss:   3.652\n",
      "epoch:  5    weight 1:   4.347    weight 2:   4.805    loss:   2.615\n",
      "epoch:  6    weight 1:   4.767    weight 2:   5.638    loss:   2.853\n",
      "epoch:  7    weight 1:   4.960    weight 2:   6.387    loss:   3.457\n",
      "epoch:  8    weight 1:   4.952    weight 2:   7.034    loss:   3.919\n",
      "epoch:  9    weight 1:   4.787    weight 2:   7.562    loss:   4.068\n",
      "epoch: 10    weight 1:   4.504    weight 2:   7.965    loss:   3.918\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "opt = optimizers.Adam(learning_rate=eta)\n",
    "train(num_epochs, opt, square_loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov-accelerated Adam (Nadam)\n",
    "* Equation\n",
    "\\begin{align}\n",
    "w^{(t)}&=w^{(t-1)}-\\frac{\\eta}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon}(\\beta_1\\hat{m}^{(t)}+(1-\\beta_1)\\hat{g}^{(t)})\\\\\n",
    "\\mathbf{g}^{(t)}&=\\nabla_\\mathbf{w}L(\\mathbf{w}^{(t)})\\\\\n",
    "G^{(t)}&=\\mathbf{g}^{(t)T}\\mathbf{g}^{(t)}\\\\\n",
    "\\mathbf{m}^{(t)}&=\\beta_1\\mathbf{m}^{(t-1)}+(1-\\beta_1)\\mathbf{g}^{(t)}\\\\\n",
    "v^{(t)}&=\\beta_2v^{(t-1)}+(1-\\beta_2)G^{(t)}\\\\\n",
    "\\mathbf{\\hat{g}}^{(t)}&=\\frac{\\mathbf{g}^{(t)}}{1-\\beta_1^t}\\\\\n",
    "\\mathbf{\\hat{m}}^{(t)}&=\\frac{\\mathbf{m}^{(t)}}{1-\\beta_1^{(t+1)}}\\\\\n",
    "\\hat{v}^{(t)}&=\\frac{v^{(t)}}{1-\\beta_2^t}\\\\\n",
    "\\end{align}\n",
    "* Parameters\n",
    "    * $\\epsilon$ is a constant that make sure the denominator is not zero. (default in Tensorflow/Keras $1e-7$).\n",
    "    * $\\beta_1$ is the parameter that control how much the model should consider the computed gradient (direction and magnitude) before. (default in Tensorflow/Keras $0.9$)\n",
    "    * $\\beta_2$ is the parameter that control how much the model should consider the computed gradient (sum of magnitude) before. (default in Tensorflow/Keras $0.999$)\n",
    "    * $\\eta$ is the learning rate (default in Tensorflow/Keras $0.001$)\n",
    "* Properties\n",
    "    * Perform similarly to Adam, but usually converge faster.\n",
    "    * Consider both magnitude and direction of previous gradient when update the parameters.\n",
    "    * Converge even with large learning rate.\n",
    "    * Less vunerable to local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0    weight 1:   0.000    weight 2:   0.000    loss:  37.729\n",
      "epoch:  1    weight 1:   1.056    weight 2:   1.056    loss:  23.121\n",
      "epoch:  2    weight 1:   1.700    weight 2:   1.769    loss:  15.800\n",
      "epoch:  3    weight 1:   2.215    weight 2:   2.394    loss:  10.844\n",
      "epoch:  4    weight 1:   2.643    weight 2:   2.979    loss:   7.311\n",
      "epoch:  5    weight 1:   2.988    weight 2:   3.534    loss:   4.814\n",
      "epoch:  6    weight 1:   3.247    weight 2:   4.056    loss:   3.091\n",
      "epoch:  7    weight 1:   3.424    weight 2:   4.541    loss:   1.927\n",
      "epoch:  8    weight 1:   3.525    weight 2:   4.985    loss:   1.156\n",
      "epoch:  9    weight 1:   3.565    weight 2:   5.381    loss:   0.659\n",
      "epoch: 10    weight 1:   3.558    weight 2:   5.727    loss:   0.355\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "w = tf.Variable(tf.zeros((2, 1)))\n",
    "opt = optimizers.Nadam(learning_rate=eta)\n",
    "train(num_epochs, opt, square_loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
