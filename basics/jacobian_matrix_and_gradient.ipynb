{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives, Jacobian Matrix and Gradient\n",
    "Suppose $\\mathbf{\\hat{y}}$ is an $m$ length vector is a function of another variable vector $\\mathbf{w}$ with length $n$ (i.e. $\\mathbf{\\hat{y}} = \\psi(\\mathbf{w})$, where $\\psi: \\mathbb{R}^n \\to \\mathbb{R}^m$). The __Jacobian matrix__ (matrix with the first-order partial derivatives) of $\\mathbf{\\hat{y}}$ with respect to $\\mathbf{w}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}}=[\\hat{y}_{i}]\n",
    "=\\begin{bmatrix}\n",
    "    \\hat{y}_{1}\\\\\n",
    "    \\hat{y}_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\hat{y}_{m}\n",
    "\\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{w}=[w_{j}]\n",
    "=\\begin{bmatrix}\n",
    "    w_{1}\\\\\n",
    "    w_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    w_{n}\n",
    "\\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{J}_\\psi(\\mathbf{w})=\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial\\mathbf{w}}=\\begin{bmatrix}\n",
    "    \\frac{\\partial\\hat{y}_1}{\\partial w_1} & \\frac{\\partial\\hat{y}_1}{\\partial w_2} & \\cdots & \\frac{\\partial\\hat{y}_1}{\\partial w_n}\\\\\n",
    "    \\frac{\\partial\\hat{y}_2}{\\partial w_1} & \\frac{\\partial\\hat{y}_2}{\\partial w_2} & \\cdots & \\frac{\\partial\\hat{y}_2}{\\partial w_n}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "    \\frac{\\partial\\hat{y}_m}{\\partial w_1} & \\frac{\\partial\\hat{y}_m}{\\partial w_2} & \\cdots & \\frac{\\partial\\hat{y}_m}{\\partial w_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For instance, let $\\mathbf{\\hat{y}}=\\mathbf{Xw} + b$, where $\\mathbf{X}$ is a matrix independent from $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=[x_{i,j}] \n",
    "=\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "    \\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{\\hat{y}}\n",
    "=\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    w_{1}\\\\\n",
    "    w_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    w_{n}\\\\\n",
    "\\end{bmatrix} + b \n",
    "=\\begin{bmatrix}\n",
    "    w_1x_{1,1} + w_2x_{1,2} + \\cdots + w_mx_{1,n} + b\\\\\n",
    "    w_1x_{2,1} + w_2x_{2,2} + \\cdots + w_mx_{2,n} + b\\\\\n",
    "    \\vdots\\\\\n",
    "    w_1x_{m,1} + w_2x_{m,2} + \\cdots + w_mx_{m,n} + b\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian matrix of $\\mathbf{\\hat{y}}$ with respect to $\\mathbf{w}$ is:\n",
    "$$\n",
    "\\mathbf{J}_{\\mathbf{Xw}+b}(\\mathbf{w})=\\mathbf{\\frac{\\partial \\hat{y}}{\\partial w}}\n",
    "=\\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\cdots & x_{1,n}\\\\\n",
    "    x_{2,1} & x_{2,2} & \\cdots & x_{2,n}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\\\\\n",
    "\\end{bmatrix} = \\mathbf{X}\n",
    "$$\n",
    "\n",
    "Suppose $f(\\mathbf{\\hat{y}})=\\mathbf{\\hat{y}}^T\\mathbf{A}\\mathbf{\\hat{y}}$, where $\\mathbf{A} \\in \\mathbb{R}^{m\\times m}$ is matrix independent from $\\mathbf{\\hat{y}}$, then $f(\\mathbf{\\hat{y}})$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{\\hat{y}})\n",
    "&=\\begin{bmatrix}\n",
    "\\hat{y}_1 & \\hat{y}_2 & \\cdots & \\hat{y}_m\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    a_{1,1} & a_{1,2} & \\cdots & a_{1,m}\\\\\n",
    "    a_{2,1} & a_{2,2} & \\cdots & a_{2,m}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    a_{m,1} & a_{m,2} & \\cdots & a_{m,m}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "    \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_m\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix} \n",
    "    \\sum\\limits_{i=1}^{m}\\hat{y}_ia_{i,1} & \\sum\\limits_{i=1}^{m}\\hat{y}_ia_{i,2} & \\cdots & \\sum\\limits_{i=1}^{m}\\hat{y}_ia_{i,m}\\\\ \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "    \\hat{y}_1 \\\\  \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_m\\\\ \n",
    "\\end{bmatrix}\\\\\n",
    "&=\\sum\\limits_{j=1}^{m}\\hat{y}_j\\sum\\limits_{i=1}^{m}\\hat{y}_ia_{i,j}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "then the __gradient__ (the partial derivatives of a function $f: \\mathbb{R}^m \\to \\mathbb{R}$ with respect to a $m$ length vector) for the above mentioned function $f(\\hat{y})$ with respect to $\\hat{y}$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\hat{y}}f\n",
    "&=\\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial\\hat{y}_1} & \\frac{\\partial f}{\\partial\\hat{y}_2} & \\cdots & \\frac{\\partial f}{\\partial\\hat{y}_m} \n",
    "\\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix}\n",
    "    2\\hat{y}_1\\sum\\limits_{i=1}^{m}a_{i,1} & 2\\hat{y}_2\\sum\\limits_{i=1}^{m}a_{i,2} & \\cdots & 2\\hat{y}_m\\sum\\limits_{i=1}^{m}a_{i,m}\n",
    "\\end{bmatrix}^T\\\\\n",
    "&=[2\\hat{y}^T\\mathbf{A}]^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Suppose we have another function that map an vector to a scalar $L(\\mathbf{\\hat{y}}) = (\\mathbf{y} - \\mathbf{\\hat{y}})^T(\\mathbf{y} - \\mathbf{\\hat{y}}$), where $\\mathbf{\\hat{y}}$ is a $m$ length vector independent from $\\mathbf{y}$ ($L$ is actually the square deviation between $\\mathbf{y}$ and $\\mathbf{\\hat{y}}$)\n",
    "\n",
    "$$\n",
    "\\mathbf{y}=[y_{i}]=\\begin{bmatrix}\n",
    "    y_{1}\\\\\n",
    "    y_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    y_{m}\\\\\n",
    "\\end{bmatrix}, \\;\\;\\;\n",
    "\\mathbf{\\hat{y}}=[\\hat{y}_{i}]=\\begin{bmatrix}\n",
    "    \\hat{y}_{1}\\\\\n",
    "    \\hat{y}_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\hat{y}_{m}\\\\\n",
    "\\end{bmatrix}=\\mathbf{Xw}+b, \\;\\;\\;\n",
    "L(\\mathbf{\\hat{y}})=(\\mathbf{y} - \\mathbf{\\hat{y}})^T(\\mathbf{y} - \\mathbf{\\hat{y}})=\\sum\\limits_{i=1}^{m}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "We can compute the gradient of this function with respect to $\\mathbf{\\hat{y}}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\hat{y}}L&=\\begin{bmatrix} \\frac{\\partial L}{\\partial\\hat{y_1}} & \\frac{\\partial L}{\\partial\\hat{y_2}} & \\cdots & \\frac{\\partial L}{\\partial\\hat{y_m}} \\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix} \n",
    "    \\frac{\\partial L}{\\partial(y_1 - \\hat{y}_1)}\\frac{\\partial(y_1 - \\hat{y}_1)}{\\partial\\hat{y}_1} & \n",
    "    \\frac{\\partial L}{\\partial(y_2 - \\hat{y}_2)}\\frac{\\partial(y_2 - \\hat{y}_2)}{\\partial\\hat{y}_2} & \n",
    "    \\cdots & \n",
    "    \\frac{\\partial L}{\\partial(y_m - \\hat{y}_m)}\\frac{\\partial(y_m - \\hat{y}_m)}{\\partial\\hat{y}_m} \\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix} \n",
    "    2(\\hat{y}_1 - y_1) & \n",
    "    2(\\hat{y}_2 - y_2) & \n",
    "    \\cdots & \n",
    "    2(\\hat{y}_m - y_m) \\end{bmatrix}^T\\\\\n",
    "&=2(\\mathbf{\\hat{y}} - \\mathbf{y})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Moreover, we can compute the gradient of $L$ with respect to $\\mathbf{w}$ using multivariate chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{w}}L&=\\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} & \\frac{\\partial L}{\\partial w_2} & \\cdots & \\frac{\\partial L}{\\partial w_n} \\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial\\mathbf{\\hat{y}}}\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w_1} & \\frac{\\partial L}{\\partial\\mathbf{\\hat{y}}}\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w_2} & \\cdots &\\frac{\\partial L}{\\partial\\mathbf{\\hat{y}}}\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial w_n} \n",
    "\\end{bmatrix}^T\\\\\n",
    "&=\\begin{bmatrix}\n",
    "    \\sum\\limits_{i=1}^{m}\\frac{\\partial L}{\\partial\\hat{y}_i}\\frac{\\partial\\hat{y}_i}{\\partial w_1} & \\sum\\limits_{i=1}^{m}\\frac{\\partial L}{\\partial\\hat{y}_i}\\frac{\\partial\\hat{y}_i}{\\partial w_2} & \\cdots & \\sum\\limits_{i=1}^{m}\\frac{\\partial L}{\\partial\\hat{y}_i}\\frac{\\partial\\hat{y}_i}{\\partial w_n} \n",
    "\\end{bmatrix}^T\\\\\n",
    "&=[2(\\mathbf{\\hat{y}} - \\mathbf{y})^T\\frac{\\partial\\mathbf{\\hat{y}}}{\\partial\\mathbf{w}}]^T\\\\\n",
    "&=[2(\\mathbf{\\hat{y}} - \\mathbf{y})^T\\mathbf{X}]^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are auto-computed gradient and self-derived gradient the same?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "X = tf.random.uniform((4, 5), minval=-1, maxval=1)\n",
    "w = tf.Variable(tf.random.uniform((5, 1), minval=-1, maxval=1))\n",
    "y = tf.Variable(tf.random.uniform((4, 1), minval=-5, maxval=5))\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    y_hat = tf.matmul(X, w)\n",
    "    loss = tf.reduce_sum(tf.pow(y - y_hat, 2))\n",
    "tf_autograd = g.gradient(loss, w)\n",
    "\n",
    "derived_grad = tf.transpose(2 * tf.matmul(tf.transpose(y_hat - y), X))\n",
    "\n",
    "print('Are auto-computed gradient and self-derived gradient the same?')\n",
    "print(f'{np.allclose(derived_grad.numpy(), tf_autograd.numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
